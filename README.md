# Research Collaboration Plan

## 0. Data pulled from memory

Research experience (from memory):

- Ph.D. candidate (2024-Present) at Hong Kong University of Science and Technology (HKUST), HKUST NLP Group. Research focuses on natural language processing and machine learning. Research interests explicitly listed: LLM Reasoning and Reinforcement Learning; Hallucination in Vision-Language Models (VLM); LLM truthfulness and Interpretability.
- B.Eng. (2020-2024) at Shanghai Jiao Tong University (SJTU).
- Research Intern at Shanghai AI Lab (June 2023 - December 2023).
- Research Intern at Tencent WXG (June 2024 - September 2024).
- Research Intern at MINIMAX (February 2025 - Present).

Publications (from memory) — title, year, and co-authors as listed in memory:

1. "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond" (2025) — First author: Junteng Liu. Co-authors: Yuanxiang Fan; Zhuo Jiang; Han Ding; Yongyi Hu; Chi Zhang; Yiqi Shi; Shitong Weng; Aili Chen; Shiqi Chen; Yunan Huang; Mozhi Zhang; Pengyu Zhao; Junjie Yan; Junxian He.
2. "On the Perception Bottleneck of VLMs for Chart Understanding" (2025) — First author: Junteng Liu. Co-authors: Weihao Zeng; Xiwen Zhang; Yijun Wang; Zifei Shan; Junxian He.
3. "On the Universal Truthfulness Hyperplane Inside LLMs" (2024) — First author: Junteng Liu. Co-authors: Shiqi Chen; Yu Cheng; Junxian He. (Published at EMNLP 2024)
4. "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation" (2024) — Authors: Shiqi Chen; Miao Xiong; Junteng Liu; Zhengxuan Wu; Teng Xiao; Siyang Gao; Junxian He. (Published at ICML 2024)
5. "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models" (2023) — Authors: Yuzhen Huang; Yuzhuo Bai; Zhihao Zhu; Junlei Zhang; Jinghan Zhang; Tangjun Su; Junteng Liu; Chuancheng Lv; Yikai Zhang; Jiayi Lei; Yao Fu; Maosong Sun; Junxian He. (Published at NeurIPS 2023)
6. "Composing Parameter-Efficient Modules with Arithmetic Operations" (2023) — Authors: Jinghan Zhang; Shiqi Chen; Junteng Liu; Junxian He. (Published at NeurIPS 2023)

> Note: All items above are copied from the memory graph. No additional publications or research experiences were added.

---

## 1. Timeline projection (publication years) and average publication rate

Publications by year (extracted from memory):

- 2023: 2 publications (C-Eval; Composing Parameter-Efficient Modules)
- 2024: 2 publications (On the Universal Truthfulness Hyperplane Inside LLMs; In-Context Sharpness as Alerts)
- 2025: 2 publications (SynLogic; On the Perception Bottleneck of VLMs for Chart Understanding)

Total publications in memory: 6
Observed years with publications: 2023, 2024, 2025 (3 distinct years)

Average publication rate (simple arithmetic mean):

- Average = Total publications / Number of years observed = 6 / 3 = 2.0 publications per year.

Math shown: (2 in 2023 + 2 in 2024 + 2 in 2025) / 3 = 6 / 3 = 2 per year.

---

## 2. Projection for next 3 years (2025-2027) — yearly targets

Using the historical average publication rate = 2 publications/year.

Projection (simple linear projection, no growth assumed):

- 2025: target = 2 publications (NOTE: memory already lists 2 publications in 2025; target matches observed)
- 2026: target = 2 publications (projected)
- 2027: target = 2 publications (projected)

Total projected for 2025-2027 = 2 * 3 = 6 publications.

Math shown: 2 publications/year * 3 years = 6 publications.

If instead you want an incremental projection accounting for the fact that 2025 already contains 2 publications and you only want forward-looking for 2026–2027, then:

- Remaining projected (2026–2027) = 2 publications/year * 2 years = 4 additional publications.

---

## 3. Cross-reference: skills in memory vs research areas from publications

Data check: The memory graph contains explicit research fields and interests (Natural Language Processing; Machine Learning; LLM Reasoning and Reinforcement Learning; Hallucination in Vision-Language Models; LLM truthfulness and Interpretability). The memory does NOT contain an explicit, enumerated "skills" list (e.g., specific programming, modeling, or analysis skills saved as "skills").

Because no explicit "skills" entity or list was present in memory, a strict cross-reference between an explicit skills list and publication-used skills cannot be completed without making assumptions. See the Assumptions section at the end for inferred / proposed skills. Below I provide a mapping between research areas (from publications and memory) and whether those areas appear in the publication record.

Research areas mentioned in memory and whether they appear in publications:

- LLM Reasoning: Appears (SynLogic, 2025 — "synthesizing verifiable reasoning data" is LLM reasoning work).
- Reinforcement Learning: NOT found in the listed publications (no publication in memory explicitly mentions reinforcement learning).
- Hallucination in Vision-Language Models (VLM hallucination): Appears (In-Context Sharpness as Alerts — hallucination mitigation; On the Perception Bottleneck of VLMs for Chart Understanding — VLM chart understanding / perception).
- LLM truthfulness and Interpretability: Appears (On the Universal Truthfulness Hyperplane Inside LLMs; In-Context Sharpness touches inner representations / hallucination mitigation).
- Natural Language Processing & Machine Learning (general): Appears (these are overarching fields for all publications listed).

Conclusion for explicit cross-reference: Because the memory lacks an explicit "skills" list, I cannot deterministically label which named skills have been actively used in published work vs which named skills haven't resulted in publications. However, when treating the research interests/areas from memory as proxies for skills, the only explicit gap is Reinforcement Learning (no publications listed that are explicitly RL-focused).

---

## 4. Proposed collaboration topics for skills not leveraged in publications

Because the memory does not include an explicit skills list, I treat the missing skill-of-concern identified above (Reinforcement Learning) as the primary gap to propose collaborations for. (See Assumptions below for other hypothetical skills and suggested collaborations if you want them.)

- Reinforcement Learning + LLM Reasoning:
  - Proposed collaboration topic: "Policy-Guided LLM Reasoning: Using Reinforcement Learning to Teach LLMs Stepwise Problem Solving and Reward Truthfulness".
  - Short description: Combine RL techniques (policy optimization, reward modeling) with logical-reasoning prompts / datasets (e.g., SynLogic-style verifiable reasoning data) to fine-tune or align LLM behavior toward stepwise, verifiable reasoning and reduce hallucination.
  - Why this fits: Memory lists both LLM reasoning and an absence of RL-targeted publications; integrating RL would leverage existing reasoning datasets and research direction.

If you would like proposals for additional inferred skills (e.g., data augmentation, prompt engineering, vision model fine-tuning, evaluation suite construction), I can add concrete collaboration topics — but these would be flagged as inferred skills (see Assumptions).

---

## 5. Collaborator wishlist (unique co-authors, collaboration counts, ranked)

Co-author collaboration counts (computed from the publications in memory). Rank is by number of collaborations with Junteng Liu, highest first.

1. Junxian He — 6 collaborations (co-author on: SynLogic 2025; Perception Bottleneck 2025; Universal Truthfulness 2024; In-Context Sharpness 2024; C-Eval 2023; Composing Parameter-Efficient Modules 2023)
2. Shiqi Chen — 4 collaborations (SynLogic 2025; Universal Truthfulness 2024; In-Context Sharpness 2024; Composing Parameter-Efficient Modules 2023)
3. Jinghan Zhang — 2 collaborations (C-Eval 2023; Composing Parameter-Efficient Modules 2023)

All other unique co-authors appear once each (alphabetical listing):

- Aili Chen (1)
- Chi Zhang (1)
- Chuancheng Lv (1)
- Han Ding (1)
- Jinghan Zhang (already counted above as 2)
- Junjie Yan (1)
- Maosong Sun (1)
- Miao Xiong (1)
- Mozhi Zhang (1)
- Pengyu Zhao (1)
- Shitong Weng (1)
- Teng Xiao (1)
- Weihao Zeng (1)
- Xiwen Zhang (1)
- Yao Fu (1)
- Yiqi Shi (1)
- Yikai Zhang (1)
- Yunan Huang (1)
- Yuzhen Huang (1)
- Yuzhuo Bai (1)
- Yu Cheng (1)
- Yuanxiang Fan (1)
- Zhengxuan Wu (1)
- Zhihao Zhu (1)
- Zifei Shan (1)
- Zhuo Jiang (1)
- ... (others listed above — all counted once)

Recommendation for wishlist ordering and outreach strategy:

- Primary collaborators to continue working with (high-priority): Junxian He; Shiqi Chen; Jinghan Zhang — because of repeated collaborations and established working relationships.
- Secondary collaborators to expand networks / domain expertise: authors who appear in domain-specific papers you want to continue (e.g., co-authors on SynLogic for logical reasoning datasets; co-authors on VLM chart understanding for vision-LM work).

---

## 6. Gaps analysis (research areas in experience but not matching publications)

From memory (research interests / experience):

- LLM Reasoning — covered by SynLogic (2025).
- Hallucination in VLMs — covered by In-Context Sharpness (2024) and Perception Bottleneck (2025).
- LLM truthfulness and Interpretability — covered by Universal Truthfulness Hyperplane (2024) and In-Context Sharpness.
- Reinforcement Learning — NO matching publication in the memory set.

Primary gap: Reinforcement Learning is explicitly listed among research interests in memory but has no corresponding publication in the memory graph.

Other possible gaps (depending on how granular you treat topics):
- Applied RL for LLM alignment (more specific than general RL) — no publications listed.

---

## Assumptions and missing/ambiguous data (explicit)

1. No explicit "skills" list found in memory: I searched the memory graph for explicit skills entries and none were present. Therefore I did NOT invent or assume any explicit skill labels for the strict cross-reference. The memory contains research fields and topics (NLP, ML, LLM reasoning, RL, VLM hallucination, truthfulness, interpretability), which I used as proxies where necessary, but these are not the same as an explicit "skills" list (e.g., Python, PyTorch, prompt engineering, data annotation, experimental design). If you want a cross-reference that lists programming / methodological skills, please provide a skills list or confirm I should infer skills from publication topics.

2. Publication completeness: I used only the publications present in the memory graph. If there are additional publications on Google Scholar or elsewhere not in memory, they were not used. (Memory contains a Google Scholar link but I did not fetch external pages.)

3. Year extraction: I used the years provided in the memory graph (2023–2025) and assumed they represent the publication year / appearance year. If some items are preprints with different official publication years, that could change the per-year counts.

4. Co-author counts: Counts are computed from the publications listed in memory. I counted each appearance of a co-author across the six publications. If the memory omitted additional co-author information or mis-attributed names, counts would differ.

5. Projection methodology: I used a simple historical-average projection (mean publications per observed year). No trend analysis, seasonality, or capacity constraints were modeled. If you prefer an exponential growth, declining, or supervisor-influenced-rate model, I can provide alternative projections.

6. Collaboration topic proposals for missing skills: I proposed one RL-related collaboration topic. Any further proposals for other (inferred) skills would be hypothetical and are labeled as such.

---

If you would like, next steps I can take (pick one or more):

- Create GitHub Issues in this repo for each proposed collaboration topic and assign priorities.
- Generate templated outreach emails/messages for top wishlist collaborators (Junxian He; Shiqi Chen; Jinghan Zhang) to propose joint work.
- Infer a skills list from publication text (e.g., model names, tooling) by fetching the cited GitHub repos or Google Scholar entries (requires permission to access external URLs).
- Produce a Gantt-style timeline (PNG or Markdown table) for the projected publication schedule.

