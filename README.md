# Research Collaboration Plan for Junteng Liu

## 1. Timeline Projection and Publication Rate

Publications (extracted years):
- 2023: C-Eval (NeurIPS 2023), Composing Parameter-Efficient Modules (NeurIPS 2023)
- 2024: On the Universal Truthfulness Hyperplane Inside LLMs (EMNLP 2024), In-Context Sharpness as Alerts (ICML 2024)
- 2025: SynLogic (ArXiv 2025), On the Perception Bottleneck of VLMs for Chart Understanding (ArXiv 2025)

Timeline (year -> number of publications):
- 2023: 2
- 2024: 2
- 2025: 2

Total publications in 2023-2025: 6 over 3 years -> average = 6 / 3 = 2.0 publications/year

## 2. Projection for 2025-2027

Using average rate = 2.0 publications/year.
- 2025: already has 2 (observed)
- 2026: projected 2 publications
- 2027: projected 2 publications

3-year projection (2025-2027) total = 2 + 2 + 2 = 6 publications.

Math shown: average = total_pubs / years = 6 / 3 = 2.0; projection per year = round(2.0) = 2.

## 3. Skills vs Research Areas

Skills listed in memory: (NOT EXPLICITLY LISTED) -- memory contains research interests and topics but no explicit "skills" list. Observed topics/skills inferred from publications and research experience: NLP, Machine Learning, LLM Reasoning, Reinforcement Learning, Vision-Language Models, Hallucination mitigation, Truthfulness, Interpretability, Parameter-efficient fine-tuning, Data synthesis for logical reasoning.

Published research areas (from publications):
- LLM truthfulness and interpretability (EMNLP 2024)
- Hallucination mitigation / inner representation (ICML 2024)
- Chinese evaluation suite / benchmark construction (NeurIPS 2023)
- Parameter-efficient modules / PEFT (NeurIPS 2023)
- Data synthesis for logical reasoning (SynLogic, 2025)
- Vision-Language Models / chart understanding (VLM, 2025)

Active skills used in publications:
- NLP (used across publications)
- Machine Learning (across publications)
- LLM reasoning / truthfulness / interpretability (EMNLP 2024)
- Hallucination mitigation (ICML 2024)
- Benchmark/evaluation design (NeurIPS 2023)
- Parameter-efficient modules (NeurIPS 2023)
- Vision-language/chart understanding (2025)
- Data synthesis for reasoning (SynLogic, 2025)

Skills mentioned but not clearly in publications: (from memory interests) Reinforcement Learning (mentioned as interest), potentially others not explicitly published.

## 4. Collaboration Topics for Unleveraged Skills

Skill: Reinforcement Learning
- Proposed collaboration topic: Combine Reinforcement Learning with LLM reasoning to develop RL-based curriculum learning for stepwise prompting and improving chain-of-thought reliability in LLMs. This project would evaluate whether RL fine-tuning on reasoning trajectories improves truthfulness.

(If other unleveraged skills are present, list them here.)

## 5. Collaborator Wishlist (co-authors frequency)

Co-authors and counts (from publications):
- Junxian He: appears in SynLogic (2025), VLM (2025), EMNLP 2024, NeurIPS 2023 (C-Eval), NeurIPS 2023 (Composing PE Modules) -> count: at least 5
- Shiqi Chen: appears in EMNLP 2024 paper, ICML 2024, NeurIPS 2023 (Composing PE Modules) -> count: at least 3
- Others (counts of 1): 
  - Yuanxiang Fan (1), Zhuo Jiang (1), Han Ding (1), Yongyi Hu (1), Chi Zhang (1), Yiqi Shi (1), Shitong Weng (1), Aili Chen (1), Yunan Huang (1), Mozhi Zhang (1), Pengyu Zhao (1), Junjie Yan (1)
  - Weihao Zeng (1), Xiwen Zhang (1), Yijun Wang (1), Zifei Shan (1)
  - Miao Xiong (1), Zhengxuan Wu (1), Teng Xiao (1), Siyang Gao (1)
  - Yuzhen Huang (1), Yuzhuo Bai (1), Zhihao Zhu (1), Junlei Zhang (1), Jinghan Zhang (1), Tangjun Su (1), Chuancheng Lv (1), Yikai Zhang (1), Jiayi Lei (1), Yao Fu (1), Maosong Sun (1)

Ranked by frequency (estimated):
1. Junxian He (5)
2. Shiqi Chen (3)
3. All others (1)

## 6. Gaps Analysis

Research experience topics: LLM Reasoning, Reinforcement Learning, Hallucination in VLMs, Truthfulness and Interpretability.

Publications cover: LLM reasoning/truthfulness, hallucination mitigation, VLM chart understanding, parameter-efficient modules, evaluation datasets, data synthesis for reasoning.

Gaps (experience topic without corresponding publication in memory):
- Reinforcement Learning: mentioned as interest but no publication explicitly about RL in memory.

## Assumptions
- Memory did not contain an explicit "skills" list; skills were inferred from research topics and publications.
- Publication years and co-author lists are taken from memory entries; if any co-authors or years are missing, they were not inventively added.
- The projection uses a simple historical average and linear projection; no growth or other model applied.
- Co-author frequency counts are based only on publications present in memory; external publications (e.g., on Google Scholar) were not consulted.
